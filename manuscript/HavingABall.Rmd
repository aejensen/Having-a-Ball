---
title: "Having a Ball: evaluating scoring streaks and game excitement using in-match trend estimation"
author: | 
  | Claus Thorn Ekstrøm and Andreas Kryger Jensen
  | Biostatistics, Institute of Public Health, University of Copenhagen
  | ekstrom@sund.ku.dk, aeje@sund.ku.dk
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontsize: 11pt  
header-includes:
  - \usepackage{bm}
  - \usepackage{amssymb}
  - \usepackage[labelfont=bf]{caption}
  - \DeclareMathOperator*{\argsup}{arg\,sup}  
  - \DeclareMathOperator*{\argmin}{arg\,min}  
  - \DeclareMathOperator*{\E}{E}
  - \DeclareMathOperator*{\Cov}{Cov}
  - \DeclareMathOperator*{\Cor}{Cor}
  - \DeclareMathOperator*{\Var}{Var}
  - \DeclareMathOperator*{\Erf}{Erf}
  - \DeclareMathOperator*{\Erfc}{Erfc}
  - \usepackage{multirow}
  - \usepackage{float}
  - \floatstyle{plaintop}
  - \restylefloat{table}
  - \usepackage[amsthm,thmmarks]{ntheorem}
  - \newtheorem{definition}{Definition}
  - \newtheorem{assumption}{Assumption}
  - \newtheorem{proposition}{Proposition}
  - \theoremstyle{nonumberplain}
  - \newtheorem{Proof}{Proof}
  - \usepackage{xcolor}
  - \newcommand{\revision}[1]{\textcolor{red}{#1}}
  - \usepackage{fontawesome5}
  - \usepackage{todonotes}
output:
  pdf_document: 
    keep_tex: no
    number_sections: yes
bibliography: bibliography.bib    
editor_options: 
  chunk_output_type: console
---

\begin{abstract}
Nu ved jeg godt nok intet om sport, men... \faBasketballBall!
\end{abstract}

\begin{center}
\textbf{Keywords:} APBRmetrics, Bayesian Statistics, Gaussian Processes, Sports Statistics, Trends
\end{center}

# Introduction

Sports analytics receive increased attention within statistics and not just for match prediction or betting but also for game evaluation, coaching purposes, and for setting strategies and tactics in future matches. 



Overall result of individual matches


We introduce the Excitement Trend Index (ETI) as an objective measure of spectator excitement in a given match. The ETI is defined as the expected number of times that the score difference changes monotonicity during a match. 

\begin{itemize}
\item Excitement defineret som skift af hvem, der er i føring
\item Vurdering af om et hold trender lige pt.
\item Identificere hvilket ``hot periods'' et hold har i løbet af en kamp til efterfølgende evaluering
\end{itemize}

Reference to Quantifying the Trendiness of Trends @ToT.

We need to reference and discuss the following papers

* @chen2020rank
* @chen2018functional
* @gabel2012random

Philosophical question: What exactly is the sampling model for a single match?

We could have a motivating figure here -- e.g. a variation of Figure 1 (see its caption).

The paper is structured as follows. In the next we introduce the Gaussian process that can be .. and derive the TDI and ETI.

In Section \ref{sec:application} we apply the our proposed methodology to analyse all 1143 matches from the 2019--2020 National Basketball Association (NBA) season. We conclude with a discussion in Section \ref{sec:discussion}. 

Materials to reproduce this manuscript and its analyses can be found at @HavingABall.

# Methods {#sec:method}

Our model is based on the observed score differences $D_m$ in a given match indexed by $m$. For each match we observe the random variables $\mathcal{D}_m = \left(t_{m_i}, D_{m_i}\right)_{0 < i \leq J_m}$ where $t_{m_1} < t_{m_2} < t_{m_i} < \ldots < t_{m_{J_m}}$ is the ordered time points at which any teams scores, $D_{m_i} = D_m(t_{m_i})$ is the associated difference in scores at time $t_{m_i}$, and $J_m$ is the total number of goals during the match. We use the convention that $D_m$ is the difference in scores of the home team with respect to the the away team, so that $D_m > 0$ means that the home team is leading. [\textbf{??? check that this is correct in the grønthøster}]

We assume that the observed match data are a noisy realization of a latent smooth, random function defined in continuous time evaluated at random time points where goals occur. Let $d_m$ be the latent functions from which the realizations $\mathcal{D}_m$ are generated. We then propose the following hierarchical model where $d_m$ is a Gaussian process and the observed data conditional on the scoring times and the latent process are independently normally distributed random variables with a match specific variance:
\begin{align}
\begin{split}
  \bm{\Theta}_m \mid \bm{\Psi}, \mathbf{t}_m &\sim H(\bm{\Psi})\\
  d_m(t) \mid \bm{\Theta}_m &\sim \mathcal{GP}(\mu_{\bm{\beta}_m}(t), C_{\bm{\theta}_m}(s, t))\\
  D_m(t_{m_i}) \mid d_m(t_{m_i}), t_{m_i}, \bm{\Theta}_m &\overset{iid}{\sim} N\left(d_m(t_{m_i}), \sigma_{m}^2\right)
\end{split}
\end{align}
where $\bm{\Theta}_m = (\bm{\beta}_m, \bm{\theta}_m, \sigma_{m}^2)$ is a vector of hyper-parameters governing the dynamics of the latent Gaussian process with a prior distribution $H$ indexed by parameters $\bm{\Psi}$, and $\mathbf{t}_m = (t_{m_1}, \ldots, t_{m_{J_m}})$ is the vector of time points where goals occurs in the match.

By properties of Gaussian processes (see e.g., @cramer1967stationary) the latent process $d_m$ and its time derivatives (assuming they exist) are distributed as a multivariate Gaussian process. We hence have that
\begin{align}
  \begin{bmatrix}d_m(s)\\ d^{\prime}_m(t)\\ d^{\prime\prime}_m(u)\end{bmatrix} \mid \bm{\Theta}_m &\sim \mathcal{GP}\left(\begin{bmatrix}\mu_{\bm{\beta}_m}(s)\\ \mu^{\prime}_{\bm{\beta}_m}(t)\\ \mu^{\prime\prime}_{\bm{\beta}_m}(u)\end{bmatrix}, \begin{bmatrix}C_{\bm{\theta}_m}(s, s^\prime) & \partial_2 C_{\bm{\theta}_m}(s, t) & \partial_2^2 C_{\bm{\theta}_m}(s, u)\\ \partial_1 C_{\bm{\theta}_m}(t, s) & \partial_1 \partial_2 C_{\bm{\theta}_m}(t, t^\prime) & \partial_1 \partial_2^2 C_{\bm{\theta}_m}(t, u)\\ \partial_1^2 C_{\bm{\theta}_m}(u, s) & \partial_1^2\partial_2 C_{\bm{\theta}_m}(u, t) & \partial_1^2 \partial_2^2 C_{\bm{\theta}_m}(u, u^\prime)\end{bmatrix}\right)
\end{align}
where $^\prime$ and $^{\prime\prime}$ denotes the first and second time derivatives, $\partial_j^k$ is the $k$'th order partial derivative with respect to the $j$'th variable

[\textbf{??? TALK ABOUR POSTERIOR HERE ???}]

The Excitement Trend Index (ETI) of a particular match, denoted $\text{ETI}_m$, is defined as the expected number of changes in monotonicity of the score differences $d_m$ conditional on the observed data from that match $\mathcal{D}_m$. This is equivalent to value of the expected number of zero-crossings of the posterior distribution of $d^\prime_m$.

Formally,
\begin{align*}
  \text{ETI}_m(\bm{\Theta}_m) = \E\left[\#\left\{t \in \mathcal{I}_m : df_m(t) = 0\right\} \mid \mathbf{D}_m, \mathbf{t}_m, \bm{\Theta}_m\right]\label{eq:ETIdef}
\end{align*}
where $\mathcal{I}_m$ is the interval of the time duration of a match i.e., $\mathcal{I}_m = [0; 48]$ minutes without overtime. The ETI is given by the intergal of the local Excitement Trend Index
\begin{align*}
  \mathrm{ETI}_m(\bm{\Theta}_m) = \int_{\mathcal{I}_m} d\mathrm{ETI}_m(t \mid \bm{\Theta}_m)\mathrm{d}t
\end{align*}
where $d\mathrm{ETI}$ is the local Excitement Trend Index given by
\begin{align*}
d\mathrm{ETI}_m(t \mid \bm{\Theta}_m) = \lambda(t \mid \Theta)\phi\left(\frac{\mu_{df}(t \mid \bm{\Theta})}{\Sigma_{df}(t,t \mid \bm{\Theta})^{1/2}}\right)\left(2\phi(\zeta(t\mid \bm{\Theta})) + \zeta(t\mid \bm{\Theta})\Erf\left(\frac{\zeta(t\mid \bm{\Theta}		)}{2^{1/2}}\right)\right)
\end{align*}
and $\phi\colon\, x \mapsto 2^{-1/2}\pi^{-1/2}\exp(-\frac{1}{2}x^2)$ is the standard normal density function, $\Erf\colon\, x \mapsto 2\pi^{-1/2}\int_0^x \exp(-u^2)\mathrm{d}u$ is the error function, and $\lambda$, $\omega$ and $\zeta$ are functions defined as
\begin{align*}
  \lambda(t \mid \Theta) &= \frac{\Sigma_{d^2\!f}(t,t \mid \bm{\Theta})^{1/2}}{\Sigma_{df}(t,t \mid \bm{\Theta})^{1/2}}\left(1-\omega(t \mid \bm{\Theta})^2\right)^{1/2}\\
  \omega(t \mid \bm{\Theta}) &= \frac{\Sigma_{df,d^2\!f}(t,t \mid \bm{\Theta})}{\Sigma_{df}(t,t \mid \bm{\Theta})^{1/2}\Sigma_{d^2\!f}(t,t \mid \bm{\Theta})^{1/2}}\\
  \zeta(t\mid \bm{\Theta}) &= \frac{\mu_{df}(t\mid \bm{\Theta})\Sigma_{d^2\!f}(t,t\mid \bm{\Theta})^{1/2}\omega(t)\Sigma_{df}(t,t\mid \bm{\Theta})^{-1/2} - \mu_{d^2\!f}(t\mid \bm{\Theta})}{\Sigma_{d^2\!f}(t,t\mid \bm{\Theta})^{1/2}\left(1 - \omega(t\mid \bm{\Theta})^2\right)^{1/2}}
\end{align*}
A derivation of this expression can be found in the supplementary material to @ToT.

The posterior distribution of the hyper-parameters given the observed data is then. We define $\widetilde{\bm{\Theta}}_m \sim P(\bm{\Theta}_m \mid \mathbf{D}_m, \bm{\Psi}_m, \mathbf{t}_m)$ hence
\begin{align*}
  \widetilde{\bm{\Theta}}_m &\sim \frac{G(\bm{\Theta}_m \mid \bm{\Psi}_m, \mathbf{t}_m) \int P(\mathbf{D}_m \mid f(\mathbf{t}_m), \bm{\Theta}_m, \bm{\Psi}_m, \mathbf{t}_m)dP(f_m(\mathbf{t}_m) \mid \bm{\Theta}_m, \bm{\Psi}_m, \mathbf{t}_m)}{\iint P(\mathbf{D}_m \mid f_m(\mathbf{t}_m), \bm{\Theta}_m, \bm{\Psi}_m, \mathbf{t}_m)dP(f_m(\mathbf{t}_m) \mid \bm{\Theta}_m, \bm{\Psi}_m, \mathbf{t}_m)dG(\bm{\Theta}_m \mid \bm{\Psi}_m, \mathbf{t}_m)}
\end{align*}

What we estimate is then the random variable $\widehat{\mathrm{ETI}}_m = \mathrm{ETI}_m(\widetilde{\bm{\Theta}}_m)$ which can be summarized by its moments or quantiles.

We need to argue that ETI for $S_a(t_{m_i}) - S_{b}(t_{m_i})$ is symmetric in $a$ and $b$ so that our choice of "reference group" in $D_m$ is not important. The reason is that we look at both up- and down-crossings at $0$ of $df_m$ so the choice of sign in $D_m$ is not relevant.

## Estimation

We have implemented the model described in the previous section in Stan [@carpenter2017stan]. 

Prior mean and covariance:
\begin{align*}
  \mu_{\bm{\beta}_m}(t) = \beta_{m}, \quad C_{\bm{\theta}_m}(s, t) = \alpha^2_m\exp\left(-\frac{(s-t)^2}{2\rho^2_m}\right)
\end{align*}
with $\alpha_m, \rho_m > 0$. \textbf{[??? Note: Infinitely differetiable sample paths. Sample path derivatives are well-defined]}

Hyper-parameters:
We used independent priors on $\bm{\Theta}_m = (\beta_m, \alpha_m, \rho_m, \sigma_m)$ of the form
\begin{align*}
G(\bm{\Theta}_m \mid \bm{\Psi}_m, \mathbf{t}_m) = G(\beta_m \mid \Psi_{\beta_m})G(\alpha_m \mid \Psi_{\alpha_m})G(\rho_m \mid \Psi_{\rho_m})G(\sigma_m \mid \Psi_{\sigma_m})
\end{align*}
where each prior is a heavy-tailed distribution with a moderate variance centered at the marginal maximum likelihood estimates. We used the following distributions
\begin{align*}
\beta_{m} \sim T\left(\widehat{\beta_m^\text{ML}}, 3, 3\right), \quad \alpha_m \sim T^+\left(\widehat{\alpha_m^\text{ML}}, 3, 3\right), \quad \rho_m \sim N^+\left(\widehat{\rho_m^\text{ML}}, 1\right), \quad \sigma_m \sim T^+\left(\widehat{\sigma_m^\text{ML}}, 3, 3\right)
\end{align*}
where $T^+(\cdot, \cdot, \mathrm{df})$ and $N^+(\cdot, \cdot)$ denotes the location-scale half T- and normal distribution functions with $\mathrm{df}$ degrees of freedom.  For each match we ran four independent Markov chains for 25,000 iterations each with half of the iterations used for warm-up. Convergence was assessed by trace plots of the MCMC draws and the potential scale reduction factor, $\widehat{R}$, of @gelman1992inference.



# Results {#sec:application}

We use data from @BBreference.

General idea: We estimate ETI for all matches in a given season and make a nice plot of the distribution of $\mathrm{ETI}_m$. Then we can rank the matches according to increasing ETI and show the running score difference for e.g., the lowest, median and highest ranked matches. Maybe a large forest plot of $\mathrm{ETI}_m$ would look impressive.

Given the posteriors $\mathrm{ETI}_m$ we can summarize them by a posterior mean and variance and then do a **meta analysis** where we adjust for game-specific fixed effects such as number of spectators, location, stratify by season and so on.


\begin{figure}[htbp]
\includegraphics{../figures/fig1.pdf}
\label{fig1}
\caption{Los Angeles Lakers at Miami Heat. October 11, 2020. \textbf{??? Maybe this figure is superfluous in the results sections. We could instead use the first two panels as motivation in the intruction ???}}
\end{figure}

\begin{figure}[htbp]
\includegraphics{../figures/fig2.pdf}
\label{fig2}
\caption{Caption}
\end{figure}

\begin{figure}[htbp]
\includegraphics{../figures/fig3.pdf}
\label{fig3}
\caption{Caption \textbf{??? should we make this a nice full page figure and include 25\% and 75\% ETI games as well ???}}
\end{figure}


# Discussion {#sec:discussion}

Some summarization goes on here. We have introduces etc...

We could define a weighted Excitement Trend Index, $\mathrm{ETI}^W_m$, so that changes in monotonicity of the score differences are i) weighted higher towards the end of the game and ii) weighted lower if one team is already far away of the other team. This motivates a weighted ETI of the form
\begin{align*}
  \mathrm{ETI}_m^\mathrm{W} = \int_{\mathcal{I}_m} w(t, |f_m(t)|)d\mathrm{ETI}_m(t)
\end{align*}
where $w$ is a bivariate weight function being increasing in its first variable and decreasing in its second variable. Such weight function could be constructed as a product of two kernel functions on $[0;48] \times \mathbb{R}_{\geq 0}$ with bandwidths based on studies of psychological perception.

A different approach would be to define team-specific excitement index nested with a match. Here we would only look at the **up**-crossings at zero of $df_m$ and we would get two excitement indices for each match $(\mathrm{ETI}_{am}, \mathrm{ETI}_{bm})$. for teams $a$ and $b$. This would somehow reflect how exciting each team were in match $m$ with respect to chancing the sign of the score differences in their favor.


# Acknowledgements {-}

# Bibliography {-}

<div id="refs"></div>

# Appendix {-}

the joint distribution of $(f, df, d^2\!f)$ conditional on $\mathbf{Y}$, $\mathbf{t}$ and the hyper-parameters $\bm{\Theta}$ evaluated at any finite vector $\mathbf{t}^\ast$ of $p$ time points is
\begin{align*}
\begin{bmatrix}f(\mathbf{t}^\ast)\\ df(\mathbf{t}^\ast)\\ d^2\!f(\mathbf{t}^\ast)\end{bmatrix} \mid \mathbf{Y}, \mathbf{t}, \bm{\Theta} \sim N\left(\bm{\mu},  \bm{\Sigma}\right)
\end{align*}
where $\bm{\mu} \in \mathbb{R}^{3p}$ is the column vector of posterior expectations and $\bm{\Sigma} \in \mathbb{R}^{3p \times 3p}$ is the joint posterior covariance matrix. Partitioning these as
\begin{align*}
  \bm{\mu} = \begin{bmatrix}\mu_{f}(\mathbf{t^\ast} \mid \bm{\Theta})\\ \mu_{df}(\mathbf{t^\ast} \mid \bm{\Theta})\\ \mu_{d^2\!f}(\mathbf{t^\ast} \mid \bm{\Theta})\end{bmatrix}, \quad \bm{\Sigma} = \begin{bmatrix}\Sigma_{f}(\mathbf{t^\ast},\mathbf{t^\ast} \mid \bm{\Theta}) &  \Sigma_{f,df}(\mathbf{t^\ast},\mathbf{t^\ast} \mid \bm{\Theta}) & \Sigma_{f,d^2\!f}(\mathbf{t^\ast},\mathbf{t^\ast} \mid \bm{\Theta})\\  \Sigma_{f,df}(\mathbf{t^\ast},\mathbf{t^\ast} \mid \bm{\Theta})^T & \Sigma_{df}(\mathbf{t^\ast},\mathbf{t^\ast} \mid \bm{\Theta}) & \Sigma_{df,d^2\!f}(\mathbf{t^\ast},\mathbf{t^\ast} \mid \bm{\Theta})\\ \Sigma_{f,d^2\!f}(\mathbf{t^\ast}, \mathbf{t^\ast} \mid \bm{\Theta})^T & \Sigma_{df,d^2\!f}(\mathbf{t^\ast}, \mathbf{t^\ast} \mid \bm{\Theta})^T & \Sigma_{d^2\!f}(\mathbf{t^\ast}, \mathbf{t^\ast} \mid \bm{\Theta})\end{bmatrix} 
\end{align*}
the individual components are given by
\begin{align*}
  \mu_{f}(\mathbf{t}^\ast \mid \bm{\Theta}) &= \mu_{\bm{\beta}}(\mathbf{t}^\ast) + C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1}\left(\mathbf{Y} - \mu_{\bm{\beta}}(\mathbf{t})\right)\\
  \mu_{df}(\mathbf{t}^\ast \mid \bm{\Theta}) &= d\mu_{\bm{\beta}}(\mathbf{t}^\ast) + \partial_1 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1}\left(\mathbf{Y} - \mu_{\bm{\beta}}(\mathbf{t})\right) \\
  \mu_{d^2\!f}(\mathbf{t}^\ast \mid \bm{\Theta}) &= d^2\!\mu_{\bm{\beta}}(\mathbf{t}^\ast) + \partial_1^2 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1}\left(\mathbf{Y} - \mu_{\bm{\beta}}(\mathbf{t})\right)\\
  \Sigma_{f}(\mathbf{t}^\ast, \mathbf{t}^\ast \mid \bm{\Theta}) &= C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t}^\ast) - C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1} C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}^\ast)\\
  \Sigma_{df}(\mathbf{t}^\ast, \mathbf{t}^\ast \mid \bm{\Theta}) &= \partial_1\partial_2C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t}^\ast) - \partial_1C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1} \partial_2C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}^\ast)\\
  \Sigma_{d^2\!f}(\mathbf{t}^\ast, \mathbf{t}^\ast \mid \bm{\Theta}) &= \partial_1^2\partial_2^2 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t}^\ast) - \partial_1^2 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1} \partial_2^2 C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}^\ast)\\
  \Sigma_{f, df}(\mathbf{t}^\ast, \mathbf{t}^\ast \mid \bm{\Theta}) &= \partial_2 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t}^\ast) - C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1} \partial_2 C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}^\ast)\\
  \Sigma_{f, d^2\!f}(\mathbf{t}^\ast, \mathbf{t}^\ast \mid \bm{\Theta}) &= \partial_2^2 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t}^\ast) - C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1} \partial_2^2 C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}^\ast)\\
  \Sigma_{df, d^2\!f}(\mathbf{t}^\ast, \mathbf{t}^\ast \mid \bm{\Theta}) &= \partial_1 \partial_2^2 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t}^\ast) - \partial_1 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1} \partial_2^2 C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}^\ast)
\end{align*}