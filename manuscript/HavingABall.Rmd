---
title: "Having a Ball: evaluating scoring streaks and game excitement using in-match trend estimation"
author: | 
  | Claus Thorn Ekstrøm and Andreas Kryger Jensen
  | Biostatistics, Institute of Public Health, University of Copenhagen
  | ekstrom@sund.ku.dk, aeje@sund.ku.dk
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontsize: 11pt  
header-includes:
  - \usepackage{bm}
  - \usepackage{amssymb}
  - \usepackage[labelfont=bf]{caption}
  - \DeclareMathOperator*{\argsup}{arg\,sup}  
  - \DeclareMathOperator*{\argmin}{arg\,min}  
  - \DeclareMathOperator*{\E}{E}
  - \DeclareMathOperator*{\Cov}{Cov}
  - \DeclareMathOperator*{\Cor}{Cor}
  - \DeclareMathOperator*{\Var}{Var}
  - \DeclareMathOperator*{\Erf}{Erf}
  - \DeclareMathOperator*{\Erfc}{Erfc}
  - \usepackage{multirow}
  - \usepackage{float}
  - \floatstyle{plaintop}
  - \restylefloat{table}
  - \usepackage[amsthm,thmmarks]{ntheorem}
  - \newtheorem{definition}{Definition}
  - \newtheorem{assumption}{Assumption}
  - \newtheorem{proposition}{Proposition}
  - \theoremstyle{nonumberplain}
  - \newtheorem{Proof}{Proof}
  - \usepackage{xcolor}
  - \newcommand{\revision}[1]{\textcolor{red}{#1}}
  - \usepackage{fontawesome5}
  - \usepackage{todonotes}
output:
  pdf_document: 
    keep_tex: no
    number_sections: yes
bibliography: bibliography.bib    
editor_options: 
  chunk_output_type: console
---

\begin{abstract}
Nu ved jeg godt nok intet om sport, men... \faBasketballBall!
\end{abstract}

\begin{center}
\textbf{Keywords:} APBRmetrics, Bayesian Statistics, Gaussian Processes, Sports Statistics, Trends
\end{center}

# Introduction

Sports analytics receive increasing attention within statistics and not just for match prediction or betting but also for game evaluation, in-game and post-game coaching purposes, and for setting strategies and tactics in future matches. 

Many popular sports such as football (soccer), basketball, boxing, table tennis, volleyball, American football, and handball involves matches between two teams or players where each team have the possibility of scoring throughout the match. Several research papers seek to predict match results (e.g., @skellam2; @groll2019; @Gu2019) or match winners for single matches (@cattelan2013bradley) in order to infer the match winner and potentially the winner of a tournament (@ekstrom2020trps; @baboota).  While the overall result is highly interesting it conveys very little information about the individual development and trends throughout the match and modeling approaches that allow a finer granularity of the score difference throughout the match is needed.

The trend in score difference between the two teams is a proxy for their underlying strengths. In particular, sustained periods of time where the score difference increases suggests that one team outperforms the other whereas periods where the teams are constantly catching up to each other suggest that the team's strengths in those periods are similar. Modeling the local trend of the score difference will therefore reflect several aspects of the game in particular the team strengths and game dynamics and momentum as they develop through the match.

Figure \ref{fig1} shows the development of the score difference for the final match of the playoffs in the 2020 National Basketball Association (NBA) series between Los Angeles Lakers and Miami Heat. Positive numbers indicate that LA Lakers are leading and the running score difference shows that the Lakers pulled ahead until the third quarter where Miami Heat started to keep up the scoring pace before overtaking the Lakers and reducing the lead.

\begin{figure}[htbp]
\includegraphics{../figures/fig1.pdf}
\caption{Los Angeles Lakers at Miami Heat. October 11, 2020 \textbf{[??? elaborate ???]}}
\label{fig1}
\end{figure}

In this paper we will consider the score difference between two teams as a latent Gaussian process and use the Trend Direction Index (TDI) from @ToT as an accomodating measure to evaluate the local probability of the *monotonicity* of the latent process at a given time point during a match. The Trend Direction Index uses a Bayesian framework to provide a direct answer to questions such as "What is the probability that the latent process (i.e., that one team is doing better than another) is increasing at a given time-point?". This will allow real-time evaluation of the score difference trend at the current time-point in the game and will provide post-game inference about the "hot" periods of a match where one team out-performed the other. Furthermore we will present the Excitement Trend Index (ETI) as an objective measure of spectator excitement in a given match. The ETI is defined as the expected number of times that the score difference changes monotonicity during a match. If the score difference changes monotonicity othen then that echos a game where both teams frequently score that the whereas a game with a low ETI will represent a one-sided match where one team is doing consistently better than the other over sustained periods of time.


Other authors have considered using continuous processes to model the score difference of matches. @gabel2012random shows that NBA basketball score difference is well described by a continuous-time anti-persistent random walk which suggests that a latent Gaussian process might be viable.

* @chen2020rank
* @chen2018functional


The paper is structured as follows. In the next section we introduce trend modeling through latent a Gaussian process and define the Trend Direction Index and Excitement Trend Index that captures the local trends in monotonicity and game excitement, respectively. In Section \ref{sec:application} we apply the our proposed methodology to analyse both the final match of the playoff as well as evaluating the game excitement distribution of the season by considering the ETI from all 1143 matches from the 2019--2020 National Basketball Association (NBA) season. We show how this distribution can be used to assess relative match excitement. We conclude with a discussion in Section \ref{sec:discussion}. Materials to reproduce this manuscript and its analyses can be found at @HavingABall.



Unused stuff for now: 
*Philosophical question: What exactly is the sampling model for a single match?
We could have a motivating figure here -- e.g. a variation of Figure 1 (see its caption).*


# Methods {#sec:method}

Our model is based on the observed score differences $D_m$ in a given match indexed by $m$. For each match we observe the random variables $\mathcal{D}_m = \left(t_{mi}, D_{mi}\right)_{0 < i \leq J_m}$ where $t_{m1} < t_{m2} < t_{mi} < \ldots < t_{m J_m}$ are the ordered time points at which any teams scores, $D_{mi} = D_m(t_{mi})$ is the associated difference in scores at time $t_{mi}$, and $J_m$ is the total number of scorings during the match. We use the convention that $D_m$ is the difference in scores of the away team with respect to the home team, so that $D_m(t) > 0$ means that the away team is leading at time $t$.

We assume that the observed match data are a noisy realization of a latent smooth, random function defined in continuous time evaluated at the random time points where goals occur. Let $d_m$ be the latent functions from which the realizations $\mathcal{D}_m$ are generated. We then propose the following hierarchical model where $d_m$ is a Gaussian process, and the observed data conditional on the scoring times and the values of the latent process at those times are independently normally distributed random variables with a match specific variance:
\begin{align}
\begin{split}
  \text{\faBasketballBall}_m \mid \bm{\Psi_m}, \mathbf{t}_m &\sim H(\bm{\Psi}_m)\\
  d_m(t) \mid \text{\faBasketballBall}_m &\sim \mathcal{GP}(\mu_{\bm{\beta}_m}(t), C_{\bm{\theta}_m}(s, t))\\
  D_m(t_{mi}) \mid d_m(t_{mi}), t_{mi}, \text{\faBasketballBall}_m &\overset{iid}{\sim} N\left(d_m(t_{mi}), \sigma_{m}^2\right)
\end{split}
\end{align}
where $\text{\faBasketballBall}_m = (\bm{\beta}_m, \bm{\theta}_m, \sigma_{m}^2)$ is a vector of hyper-parameters governing the dynamics of the latent Gaussian process with a prior distribution $H$ indexed by parameters $\bm{\Psi}$, and $\mathbf{t}_m = (t_{m1}, \ldots, t_{m J_m})$ is the vector of time points where goals occurs in the match.

[\textbf{Beautiful!}]


By properties of Gaussian processes (see e.g., @cramer1967stationary) the latent process $d_m$ and its time derivatives (assuming they exist) are distributed as a multivariate Gaussian process. We hence have that
\begin{align}
  \begin{bmatrix}d_m(s)\\ d^{\prime}_m(t)\\ d^{\prime\prime}_m(u)\end{bmatrix} \mid \text{\faBasketballBall}_m &\sim \mathcal{GP}\left(\begin{bmatrix}\mu_{\bm{\beta}_m}(s)\\ \mu^{\prime}_{\bm{\beta}_m}(t)\\ \mu^{\prime\prime}_{\bm{\beta}_m}(u)\end{bmatrix}, \begin{bmatrix}C_{\bm{\theta}_m}(s, s^\prime) & \partial_2 C_{\bm{\theta}_m}(s, t) & \partial_2^2 C_{\bm{\theta}_m}(s, u)\\ \partial_1 C_{\bm{\theta}_m}(t, s) & \partial_1 \partial_2 C_{\bm{\theta}_m}(t, t^\prime) & \partial_1 \partial_2^2 C_{\bm{\theta}_m}(t, u)\\ \partial_1^2 C_{\bm{\theta}_m}(u, s) & \partial_1^2\partial_2 C_{\bm{\theta}_m}(u, t) & \partial_1^2 \partial_2^2 C_{\bm{\theta}_m}(u, u^\prime)\end{bmatrix}\right)
\end{align}
where $^\prime$ and $^{\prime\prime}$ denote the first and second time derivatives, $\partial_j^k$ is the $k$'th order partial derivative with respect to the $j$'th variable

[\textbf{??? TALK ABOUR POSTERIOR HERE ??? ... Maybe move it up and discuss it right after the bayesian definition. That hsoul make it clearer early on what we can use it for.}]

The Excitement Trend Index (ETI) of a particular match, denoted $\text{ETI}_m$, is defined as the expected number of changes in monotonicity of the score differences $d_m$ conditional on the observed data from match $\mathcal{D}_m$. This is equivalent to the expected number of zero-crossings of the posterior distribution of $d^\prime_m$.

Formally,
\begin{align*}
  \text{ETI}_m(\bm{\Theta}_m) = \E\left[\#\left\{t \in \mathcal{I}_m : df_m(t) = 0\right\} \mid \mathbf{D}_m, \mathbf{t}_m, \bm{\Theta}_m\right]\label{eq:ETIdef}
\end{align*}
where $\mathcal{I}_m$ is the interval of the time duration of a match. [\textbf{Vi har ikke nævnt basketball endnu, så lader det stadig være generelt og flytter tidsrammen til results}]
The ETI is given by the integral of the local Excitement Trend Index
\begin{align*}
  \mathrm{ETI}_m(\bm{\Theta}_m) = \int_{\mathcal{I}_m} d\mathrm{ETI}_m(t \mid \bm{\Theta}_m)\mathrm{d}t
\end{align*}
where $d\mathrm{ETI}$ is the local Excitement Trend Index given by
\begin{align*}
d\mathrm{ETI}_m(t \mid \bm{\Theta}_m) = \lambda(t \mid \Theta)\phi\left(\frac{\mu_{df}(t \mid \bm{\Theta})}{\Sigma_{df}(t,t \mid \bm{\Theta})^{1/2}}\right)\left(2\phi(\zeta(t\mid \bm{\Theta})) + \zeta(t\mid \bm{\Theta})\Erf\left(\frac{\zeta(t\mid \bm{\Theta}		)}{2^{1/2}}\right)\right)
\end{align*}
and $\phi\colon\, x \mapsto 2^{-1/2}\pi^{-1/2}\exp(-\frac{1}{2}x^2)$ is the standard normal density function, $\Erf\colon\, x \mapsto 2\pi^{-1/2}\int_0^x \exp(-u^2)\mathrm{d}u$ is the error function, and $\lambda$, $\omega$ and $\zeta$ are functions defined as
\begin{align*}
  \lambda(t \mid \Theta) &= \frac{\Sigma_{d^2\!f}(t,t \mid \bm{\Theta})^{1/2}}{\Sigma_{df}(t,t \mid \bm{\Theta})^{1/2}}\left(1-\omega(t \mid \bm{\Theta})^2\right)^{1/2}\\
  \omega(t \mid \bm{\Theta}) &= \frac{\Sigma_{df,d^2\!f}(t,t \mid \bm{\Theta})}{\Sigma_{df}(t,t \mid \bm{\Theta})^{1/2}\Sigma_{d^2\!f}(t,t \mid \bm{\Theta})^{1/2}}\\
  \zeta(t\mid \bm{\Theta}) &= \frac{\mu_{df}(t\mid \bm{\Theta})\Sigma_{d^2\!f}(t,t\mid \bm{\Theta})^{1/2}\omega(t)\Sigma_{df}(t,t\mid \bm{\Theta})^{-1/2} - \mu_{d^2\!f}(t\mid \bm{\Theta})}{\Sigma_{d^2\!f}(t,t\mid \bm{\Theta})^{1/2}\left(1 - \omega(t\mid \bm{\Theta})^2\right)^{1/2}}
\end{align*}
A derivation of this expression can be found in the supplementary material to @ToT.

The posterior distribution of the hyper-parameters given the observed data is then. We define $\widetilde{\bm{\Theta}}_m \sim P(\bm{\Theta}_m \mid \mathbf{D}_m, \bm{\Psi}_m, \mathbf{t}_m)$ hence
\begin{align*}
  \widetilde{\bm{\Theta}}_m &\sim \frac{G(\bm{\Theta}_m \mid \bm{\Psi}_m, \mathbf{t}_m) \int P(\mathbf{D}_m \mid f(\mathbf{t}_m), \bm{\Theta}_m, \bm{\Psi}_m, \mathbf{t}_m)dP(f_m(\mathbf{t}_m) \mid \bm{\Theta}_m, \bm{\Psi}_m, \mathbf{t}_m)}{\iint P(\mathbf{D}_m \mid f_m(\mathbf{t}_m), \bm{\Theta}_m, \bm{\Psi}_m, \mathbf{t}_m)dP(f_m(\mathbf{t}_m) \mid \bm{\Theta}_m, \bm{\Psi}_m, \mathbf{t}_m)dG(\bm{\Theta}_m \mid \bm{\Psi}_m, \mathbf{t}_m)}
\end{align*}

What we estimate is then the random variable $\widehat{\mathrm{ETI}}_m = \mathrm{ETI}_m(\widetilde{\bm{\Theta}}_m)$ which can be summarized by its moments or quantiles.

We need to argue that ETI for $S_a(t_{m_i}) - S_{b}(t_{m_i})$ is symmetric in $a$ and $b$ so that our choice of "reference group" in $D_m$ is not important. The reason is that we look at both up- and down-crossings at $0$ of $df_m$ so the choice of sign in $D_m$ is not relevant.

## Estimation

We have implemented the model described in the previous section in Stan [@carpenter2017stan]. 

Prior mean and covariance:
\begin{align*}
  \mu_{\bm{\beta}_m}(t) = \beta_{m}, \quad C_{\bm{\theta}_m}(s, t) = \alpha^2_m\exp\left(-\frac{(s-t)^2}{2\rho^2_m}\right)
\end{align*}
with $\alpha_m, \rho_m > 0$. \textbf{[??? Note: Infinitely differetiable sample paths. Sample path derivatives are well-defined]}

Hyper-parameters:
We used independent priors on $\bm{\Theta}_m = (\beta_m, \alpha_m, \rho_m, \sigma_m)$ of the form
\begin{align*}
G(\bm{\Theta}_m \mid \bm{\Psi}_m, \mathbf{t}_m) = G(\beta_m \mid \Psi_{\beta_m})G(\alpha_m \mid \Psi_{\alpha_m})G(\rho_m \mid \Psi_{\rho_m})G(\sigma_m \mid \Psi_{\sigma_m})
\end{align*}
where each prior is a heavy-tailed distribution with a moderate variance centered at the marginal maximum likelihood estimates. We used the following distributions
\begin{align*}
\beta_{m} \sim T\left(\widehat{\beta_m^\text{ML}}, 3, 3\right), \quad \alpha_m \sim T^+\left(\widehat{\alpha_m^\text{ML}}, 3, 3\right), \quad \rho_m \sim N^+\left(\widehat{\rho_m^\text{ML}}, 1\right), \quad \sigma_m \sim T^+\left(\widehat{\sigma_m^\text{ML}}, 3, 3\right)
\end{align*}
where $T^+(\cdot, \cdot, \mathrm{df})$ and $N^+(\cdot, \cdot)$ denotes the location-scale half T- and normal distribution functions with $\mathrm{df}$ degrees of freedom.  For each match we ran four independent Markov chains for 25,000 iterations each with half of the iterations used for warm-up. Convergence was assessed by trace plots of the MCMC draws and the potential scale reduction factor, $\widehat{R}$, of @gelman1992inference.



# Application: The 2019--2020 NBA basketball season {#sec:application}

To show the applicability of our proposed model we will apply it to data from all regular games from the 2019--2020 NBA basketball season (obtained from @BBreference and provided in @HavingABall). A lot of points are scored during a basketball match so it is easy to see the development of the score difference. 

The 2019--2020 NBA season was suspended mid-March because of Covid-19 but it was resumed again in July 2020. There were a total of 1059 regular season matches. The subsequent playoffs comprised 84 matches including thee final for a grand total of 1143 matches. For ease of comparison we are only considering the first 48 regular minutes of each match --- any part of a match that goes into overtime will be disregarded. Thus, $\mathcal{I}_m = [0; 48]$ minutes without overtime.

\begin{figure}[htbp]
\includegraphics{../figures/fig2.pdf}
\caption{Histogram and kernel density estimate of the distribution of 1143 median posterior Excitement Trend Indices from the NBA 2019--2020 season (left panel) and the median posterios ETIs as a function of calender time from October 22, 2019 to October 11, 2020 (right panel).}
\label{fig2}
\end{figure}

The left panel of Figure \ref{fig2} shows the distribution of the median posterior Excitement Trend Indices estimated using our model for each of the 1143 games in the 2019--2020 NBA season. The distribution of game excitements is right skewed (skewness = 0.53) with a range of $[0.23; 26.28]$, and an median of 10.09 (mean = 10.62, SD = 4.52). This implies that the time-varying score differences of the games in the season changes monotonicity approximately 10 times during a game on average. The right panel of the same figure shows the values of the median posterior ETIs as a function of the calendar time were the matches were played. Besides illustrating the gap from the Covid-19 hiatus, the figure shows that the excitement indices are relatively evenly distributed throughout the season.

Summarizing the posterior median ETIs at the team level lead to comparable values across all 30 teams. The New Orleans Pelicans had the highest average posterior median ETI during the season with a value of 11.67 (SD = 4.91, IQR = $[3.09; 23.57]$), while the Charlotte Hornets had the lowest average posterior median ETI during the season with a value of 9.29 (SD = 3.74, IQR = $[1.96; 15.98]$). This implies that each team played games during the season that where comparable on average in terms their excitement, and the major source of variation in excitement is governed by the individual matches. The supplementary material to this paper provides summary statistics for all 30 teams.

The asymmetry of the distribution of the Excitement Trend Indices in Figure \ref{fig2} suggests the possibility of a latent, categorical variable labeling the matches to different classes. We thus fitted a heteroscedastic Gaussian mixture model to the median posterior Excitement Trend Indices on the match level, where the number of latent classes was determined by a bootstrapped likelihood ratio test using 10,000 bootstrap replicates (@mclust). Testing for 1 vs 2 latent classes gave a p-value of $< 0.001$, 2 vs 3 gave a p-value of $0.027$, 3 vs 4 gave a p-value of $0.008$, and 4 vs 5 gave a p-value of $0.300$ implying that the distribution of median posterior Excitement Trend Indices can be modeled by a 4-component Gaussian mixture distribution. The following table shows the maximum likelihood estimates of the parameters of the Gaussian mixture model.

|           | class 1| class 2| class 3| class 4|
|:----------|-------:|-------:|-------:|-------:|
|$\pi$      |    0.24|    0.22|    0.28|    0.26|
|$\mu$      |    5.70|    8.89|   12.64|   14.45|
|$\sigma^2$ |    4.44|    1.87|    5.15|   23.31|

Figure \ref{fig2-5} shows the estimated Gaussian mixture model along with the histogram (left panel), and the proportion of matches classified into the four latent classes as determined by a maximum posterior decision rule (right panel). 

[\textbf{So we basically have a group of average matches (class 2), a group below average (class 1), a group above average (class 3) and then also group of 150 matches with an even large average but also a larger variance (class 4). Can we say something about these classes?}]

\begin{figure}[htbp]
\includegraphics{../figures/fig2-5.pdf}
\caption{}
\label{fig2-5}
\end{figure}



Figure \ref{fig3} shows ...

\begin{figure}[htbp]
\includegraphics{../figures/fig3.pdf}
\caption{Observed score differences with posteriors of the latent processes (left panels), posterior trends (middle panels) and posterior Trend Direction Indices (right panels) for five games from the NBA season 2019--2020 with median posterior Excitement Trend Indices corresponding to the 0\%, 25\%, 50\%, 75\%, and 100\% percentiles of the distribution of all games in the season. Gray regions depict 50\%, 95\%, and 99\% point-wise credible intervals.}
\label{fig3}
\end{figure}


# Discussion {#sec:discussion}

Some summarization goes on here. We have introduced etc...

We could define a weighted Excitement Trend Index, $\mathrm{ETI}^\mathrm{W}_m$, so that changes in monotonicity of the score differences are i) weighted higher towards the end of the game and ii) weighted lower if one team is already far away of the other team. This motivates a weighted ETI of the form
\begin{align*}
  \mathrm{ETI}_m^\mathrm{W} = \int_{\mathcal{I}_m} w(t, |f_m(t)|)d\mathrm{ETI}_m(t)
\end{align*}
where $w$ is a bivariate weight function being increasing in its first variable and decreasing in its second variable. Such weight function could be constructed as a product of two kernel functions on $[0;48] \times \mathbb{R}_{\geq 0}$ with bandwidths based on studies of psychological perception.

A different approach would be to define team-specific excitement index nested with a match. Here we would only look at the **up**-crossings at zero of $df_m$ and we would get two excitement indices for each match $(\mathrm{ETI}_{am}, \mathrm{ETI}_{bm})$. for teams $a$ and $b$. This would somehow reflect how exciting each team were in match $m$ with respect to chancing the sign of the score differences in their favor.


# Acknowledgements {-}

# Bibliography {-}

<div id="refs"></div>

# Appendix (move to supplementary) {-}

the joint distribution of $(f, df, d^2\!f)$ conditional on $\mathbf{Y}$, $\mathbf{t}$ and the hyper-parameters $\bm{\Theta}$ evaluated at any finite vector $\mathbf{t}^\ast$ of $p$ time points is
\begin{align*}
\begin{bmatrix}f(\mathbf{t}^\ast)\\ df(\mathbf{t}^\ast)\\ d^2\!f(\mathbf{t}^\ast)\end{bmatrix} \mid \mathbf{Y}, \mathbf{t}, \bm{\Theta} \sim N\left(\bm{\mu},  \bm{\Sigma}\right)
\end{align*}
where $\bm{\mu} \in \mathbb{R}^{3p}$ is the column vector of posterior expectations and $\bm{\Sigma} \in \mathbb{R}^{3p \times 3p}$ is the joint posterior covariance matrix. Partitioning these as
\begin{align*}
  \bm{\mu} = \begin{bmatrix}\mu_{f}(\mathbf{t^\ast} \mid \bm{\Theta})\\ \mu_{df}(\mathbf{t^\ast} \mid \bm{\Theta})\\ \mu_{d^2\!f}(\mathbf{t^\ast} \mid \bm{\Theta})\end{bmatrix}, \quad \bm{\Sigma} = \begin{bmatrix}\Sigma_{f}(\mathbf{t^\ast},\mathbf{t^\ast} \mid \bm{\Theta}) &  \Sigma_{f,df}(\mathbf{t^\ast},\mathbf{t^\ast} \mid \bm{\Theta}) & \Sigma_{f,d^2\!f}(\mathbf{t^\ast},\mathbf{t^\ast} \mid \bm{\Theta})\\  \Sigma_{f,df}(\mathbf{t^\ast},\mathbf{t^\ast} \mid \bm{\Theta})^T & \Sigma_{df}(\mathbf{t^\ast},\mathbf{t^\ast} \mid \bm{\Theta}) & \Sigma_{df,d^2\!f}(\mathbf{t^\ast},\mathbf{t^\ast} \mid \bm{\Theta})\\ \Sigma_{f,d^2\!f}(\mathbf{t^\ast}, \mathbf{t^\ast} \mid \bm{\Theta})^T & \Sigma_{df,d^2\!f}(\mathbf{t^\ast}, \mathbf{t^\ast} \mid \bm{\Theta})^T & \Sigma_{d^2\!f}(\mathbf{t^\ast}, \mathbf{t^\ast} \mid \bm{\Theta})\end{bmatrix} 
\end{align*}
the individual components are given by
\begin{align*}
  \mu_{f}(\mathbf{t}^\ast \mid \bm{\Theta}) &= \mu_{\bm{\beta}}(\mathbf{t}^\ast) + C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1}\left(\mathbf{Y} - \mu_{\bm{\beta}}(\mathbf{t})\right)\\
  \mu_{df}(\mathbf{t}^\ast \mid \bm{\Theta}) &= d\mu_{\bm{\beta}}(\mathbf{t}^\ast) + \partial_1 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1}\left(\mathbf{Y} - \mu_{\bm{\beta}}(\mathbf{t})\right) \\
  \mu_{d^2\!f}(\mathbf{t}^\ast \mid \bm{\Theta}) &= d^2\!\mu_{\bm{\beta}}(\mathbf{t}^\ast) + \partial_1^2 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1}\left(\mathbf{Y} - \mu_{\bm{\beta}}(\mathbf{t})\right)\\
  \Sigma_{f}(\mathbf{t}^\ast, \mathbf{t}^\ast \mid \bm{\Theta}) &= C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t}^\ast) - C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1} C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}^\ast)\\
  \Sigma_{df}(\mathbf{t}^\ast, \mathbf{t}^\ast \mid \bm{\Theta}) &= \partial_1\partial_2C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t}^\ast) - \partial_1C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1} \partial_2C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}^\ast)\\
  \Sigma_{d^2\!f}(\mathbf{t}^\ast, \mathbf{t}^\ast \mid \bm{\Theta}) &= \partial_1^2\partial_2^2 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t}^\ast) - \partial_1^2 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1} \partial_2^2 C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}^\ast)\\
  \Sigma_{f, df}(\mathbf{t}^\ast, \mathbf{t}^\ast \mid \bm{\Theta}) &= \partial_2 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t}^\ast) - C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1} \partial_2 C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}^\ast)\\
  \Sigma_{f, d^2\!f}(\mathbf{t}^\ast, \mathbf{t}^\ast \mid \bm{\Theta}) &= \partial_2^2 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t}^\ast) - C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1} \partial_2^2 C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}^\ast)\\
  \Sigma_{df, d^2\!f}(\mathbf{t}^\ast, \mathbf{t}^\ast \mid \bm{\Theta}) &= \partial_1 \partial_2^2 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t}^\ast) - \partial_1 C_{\bm{\theta}}(\mathbf{t}^\ast, \mathbf{t})\left(C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}) + \sigma^2 I\right)^{-1} \partial_2^2 C_{\bm{\theta}}(\mathbf{t}, \mathbf{t}^\ast)
\end{align*}